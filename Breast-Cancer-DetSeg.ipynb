{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4065a03",
   "metadata": {},
   "source": [
    "<h1 dir=ltr align=center style=\"line-height:200%;font-family:sans-serif;color:#0099cc\">\n",
    "<font face=\"sans-serif\" color=\"#0099cc\">\n",
    "Ultravision Operations\n",
    "</font>\n",
    "</h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88503a64",
   "metadata": {},
   "source": [
    "<h2 dir=ltr align=left style=\"line-height:200%;font-family:sans-serif;color:#0099cc\">\n",
    "<font face=\"sans-serif\" color=\"#0099cc\">\n",
    "Introduction and Problem Statement\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "\n",
    "<p dir=ltr style=\"direction: ltr; text-align: justify; line-height:200%; font-family:sans-serif; font-size:medium\">\n",
    "<font face=\"sans-serif\">\n",
    "Welcome to the final stage of the Quera Image Processing and Computer Vision Olympiad! A place where pixels find meaning, models make decisions, and you are the commander of a great scientific mission. In this stage, you will face a multi-part project; a mission that challenges your intelligence, precision, and creativity in the segmentation, classification, and analysis of medical images.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f6ce10f",
   "metadata": {},
   "source": [
    "<h2 dir=ltr align=left style=\"line-height:200%;font-family:sans-serif;color:#0099cc\">\n",
    "<font face=\"sans-serif\" color=\"#0099cc\">\n",
    "Dataset Introduction\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=ltr style=\"direction: ltr; text-align: justify; line-height:200%; font-family:sans-serif; font-size:medium\">\n",
    "<font face=\"sans-serif\" size=3>\n",
    " In the project's root folder, there are two folders named <code>train</code> and <code>test</code>.\n",
    "Each folder contains the images for the training and test sets.\n",
    "\n",
    "<p dir=ltr style=\"direction: ltr; text-align: justify; line-height:200%; font-family:sans-serif; font-size:medium\">\n",
    "<font face=\"sans-serif\" size=3>\n",
    "The train folder contains all the training images. In this folder, there is another file named <code>train.csv</code>, in which the label for each image is specified in <code>One_Hot</code> format.\n",
    "\n",
    "<p dir=ltr style=\"direction: ltr; text-align: justify; line-height:200%; font-family:sans-serif; font-size:medium\">\n",
    "<font face=\"sans-serif\" size=3>\n",
    " The training set includes three classes: benign, malignant, and normal. Within each class folder, there are two separate subfolders named images and masks; the images folder contains the original ultrasound images, and the masks folder contains the corresponding segmented masks for those images. The name of the mask file is exactly the same as the original image, with only the suffix _mask added to the end of its name (e.g., the image benign (1).png has the mask benign (1)_mask.png). This structure allows you to easily access the images and masks for each class in the training sets and use them to train and evaluate classification or segmentation models.\n",
    "\n",
    "<p dir=ltr style=\"direction: ltr; text-align: justify; line-height:200%; font-family:sans-serif; font-size:medium\">\n",
    "<font face=\"sans-serif\" size=3>\n",
    "The test folder also contains all the test images. Additionally, a file named <code>test.csv</code> is located in this folder. This file contains the names of the test set images, but their labels are unknown. You must predict the label for each image in the order they appear in this file.\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68640bdc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "887d7411",
   "metadata": {},
   "source": [
    "<h4 dir=ltr align=left style=\"line-height:200%;font-family:sans-serif;color:#0099cc\">\n",
    "<font face=\"sans-serif\" color=\"#0099cc\">\n",
    "Introduction to the Training Dataset (train)\n",
    "</font>\n",
    "</h4>\n",
    "\n",
    "<div dir=\"ltr\">\n",
    "<p dir=ltr style=\"direction: ltr; text-align: justify; line-height:200%; font-family:sans-serif; font-size:medium\">\n",
    "<font face=\"sans-serif\" size=3>\n",
    "\n",
    "The <code>train</code> set has 624 images with average dimensions of 500x500. The segmented images for each class are located in the mask folder for that category.\n",
    "\n",
    "The <code>train.csv</code> file is where the name of each image and its label are specified in <code>one_hot</code> format. This file is as follows.\n",
    "\n",
    "<center>\n",
    "<div dir=ltr style=\"direction: ltr;line-height:200%;font-family:sans-serif;font-size:medium\">\n",
    "<font face=\"sans-serif\" size=3>\n",
    "    \n",
    "\n",
    "| Column Name | Explanation |\n",
    "|:---: |:---: |\n",
    "| `image` | Image name|\n",
    "| `class_benign` | Image contains a benign tumor. |\n",
    "| `class_malignant` |Image contains a malignant tumor.|\n",
    "| `class_normal` |Image has no tumor.|\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9db7bc8",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4e3bebd0",
   "metadata": {},
   "source": [
    "<h4 dir=ltr align=left style=\"line-height:200%;font-family:sans-serif;color:#0099cc\">\n",
    "<font face=\"sans-serif\" color=\"#0099cc\">\n",
    "Introduction to the Test Dataset (test)\n",
    "</font>\n",
    "</h4>\n",
    "\n",
    "<div dir=\"ltr\">\n",
    "<p dir=ltr style=\"direction: ltr; text-align: justify; line-height:200%; font-family:sans-serif; font-size:medium\">\n",
    "<font face=\"sans-serif\" size=3>\n",
    "\n",
    "The <code>test</code> set has 156 images with dimensions of 500x500.\n",
    "\n",
    "In the <code>test.csv</code> file, the names of the test set images are listed. At the end, you must make your predictions based on this file. This file is as follows:\n",
    "\n",
    "<center>\n",
    "<div dir=ltr style=\"direction: ltr;line-height:200%;font-family:sans-serif;font-size:medium\">\n",
    "<font face=\"sans-serif\" size=3>\n",
    "    \n",
    "\n",
    "| Column Name | Explanation |\n",
    "|:---: |:---: |\n",
    "| `image` | Image name|\n",
    "\n",
    "\n",
    "</font>\n",
    "</div>\n",
    "</center>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2d3c22",
   "metadata": {},
   "source": [
    "<h2 dir=ltr align=left style=\"line-height:200%;font-family:sans-serif;color:#0099cc\">\n",
    "<font face=\"sans-serif\" color=\"#0099cc\">\n",
    "Part One: Classification using Vision-Language Models\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=ltr style=\"direction: ltr; text-align: justify; line-height:200%; font-family:sans-serif; font-size:medium\">\n",
    "<font face=\"sans-serif\" size=3>\n",
    "As described, place the code for designing a vision-language model in this section. Your training results must be evident in this section.\n",
    "</font>\n",
    "</p>\n"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:30:34.424180Z",
     "start_time": "2025-11-01T10:30:34.321747Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy\n",
    "print(numpy.__version__) # make sure your numpy version < 2.0.0"
   ],
   "id": "a18e648433a09ef0",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.26.4\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:30:43.436217Z",
     "start_time": "2025-11-01T10:30:35.167883Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score, classification_report\n",
    "import clip\n",
    "import glob\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from tqdm.notebook import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch.optim as optim\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import segmentation_models_pytorch as smp\n",
    "import zipfile\n",
    "import os"
   ],
   "id": "c4e480e029902609",
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:30:43.445212Z",
     "start_time": "2025-11-01T10:30:43.436217Z"
    }
   },
   "cell_type": "code",
   "source": "df = pd.read_csv('train/train.csv')",
   "id": "a2efc8676f17bbaf",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-11-01T10:30:43.462995Z",
     "start_time": "2025-11-01T10:30:43.445212Z"
    }
   },
   "cell_type": "code",
   "source": "df",
   "id": "65c6784ba4061a3a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                   image  class_benign  class_malignant  class_normal\n",
       "0     malignant (88).png           0.0              1.0           0.0\n",
       "1        benign (81).png           1.0              0.0           0.0\n",
       "2    malignant (116).png           0.0              1.0           0.0\n",
       "3       benign (331).png           1.0              0.0           0.0\n",
       "4       benign (319).png           1.0              0.0           0.0\n",
       "..                   ...           ...              ...           ...\n",
       "619      normal (63).png           0.0              0.0           1.0\n",
       "620     benign (214).png           1.0              0.0           0.0\n",
       "621    malignant (7).png           0.0              1.0           0.0\n",
       "622     benign (400).png           1.0              0.0           0.0\n",
       "623     benign (215).png           1.0              0.0           0.0\n",
       "\n",
       "[624 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class_benign</th>\n",
       "      <th>class_malignant</th>\n",
       "      <th>class_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>malignant (88).png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>benign (81).png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>malignant (116).png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>benign (331).png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>benign (319).png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>normal (63).png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>benign (214).png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>malignant (7).png</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>benign (400).png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>benign (215).png</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:40:38.354334Z",
     "start_time": "2025-10-30T09:40:38.350331Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 1. Settings and Configuration ---\n",
    "class Config:\n",
    "    DATA_PATH = \"train/\" # Path to the train folder\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "    CLIP_MODEL = \"ViT-B/32\"\n",
    "    EPOCHS = 10\n",
    "    BATCH_SIZE = 32\n",
    "    LEARNING_RATE = 1e-5 # Low learning rate for fine-tuning\n",
    "    VALIDATION_SPLIT = 0.2 # 20% for validation\n",
    "    RANDOM_SEED = 42"
   ],
   "id": "3e54227c2663680",
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:40:39.109227Z",
     "start_time": "2025-10-30T09:40:39.037282Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Initial setup\n",
    "cfg = Config()\n",
    "torch.manual_seed(cfg.RANDOM_SEED)\n",
    "np.random.seed(cfg.RANDOM_SEED)"
   ],
   "id": "edad966d6933ce81",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:40:39.565081Z",
     "start_time": "2025-10-30T09:40:39.561264Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 2. Data Preparation (Dataset and DataLoader) ---\n",
    "class XRayDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Custom Dataset class for loading images and labels.\n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, preprocess):\n",
    "        self.df = dataframe\n",
    "        self.preprocess = preprocess\n",
    "        self.class_to_idx = {\"normal\": 0, \"benign\": 1, \"malignant\": 2}\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        label_name = row['label']\n",
    "        label_idx = self.class_to_idx[label_name]\n",
    "\n",
    "        # Load image\n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            # Apply CLIP-required preprocessing\n",
    "            image_tensor = self.preprocess(image)\n",
    "            return image_tensor, label_idx\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {image_path}: {e}\")\n",
    "            # In case of error, return an empty image and label\n",
    "            \n",
    "            return torch.zeros((3, 224, 224)), -1 "
   ],
   "id": "39ebd16ac6f916cd",
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:40:40.149020Z",
     "start_time": "2025-10-30T09:40:40.145407Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_dataframe(data_path):\n",
    "    \"\"\"\n",
    "    Traverse folders and build a DataFrame of image paths and labels.\n",
    "    \"\"\"\n",
    "    classes = [\"normal\", \"benign\", \"malignant\"]\n",
    "    data = []\n",
    "    for cls in classes:\n",
    "        img_dir = os.path.join(data_path, cls, \"images\")\n",
    "        if not os.path.exists(img_dir):\n",
    "            print(f\"Warning: Directory not found: {img_dir}\")\n",
    "            continue\n",
    "            \n",
    "        for img_name in os.listdir(img_dir):\n",
    "            if img_name.endswith(\".png\"):\n",
    "                data.append({\n",
    "                    \"image_path\": os.path.join(img_dir, img_name),\n",
    "                    \"label\": cls\n",
    "                })\n",
    "    \n",
    "    if not data:\n",
    "        raise FileNotFoundError(f\"No PNG images found in {data_path}. Check your path and structure.\")\n",
    "        \n",
    "    return pd.DataFrame(data)"
   ],
   "id": "bf4e297d834742cd",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:40:40.553906Z",
     "start_time": "2025-10-30T09:40:40.550070Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_class_weights(dataframe):\n",
    "    \"\"\"\n",
    "    Calculate inverse class weights for the loss function.\n",
    "    \"\"\"\n",
    "    class_counts = dataframe['label'].value_counts().sort_index()\n",
    "    class_map = {\"normal\": 0, \"benign\": 1, \"malignant\": 2}\n",
    "    \n",
    "    # Ensure correct order\n",
    "    counts = [class_counts.get(cls, 1) for cls in class_map.keys()] # 1 to prevent division by zero\n",
    "    \n",
    "    total = sum(counts)\n",
    "    weights = [total / (len(counts) * count) for count in counts]\n",
    "    \n",
    "    # Normalize weights\n",
    "    weights_tensor = torch.tensor(weights, dtype=torch.float32).to(cfg.DEVICE)\n",
    "    print(f\"Class Counts: {counts}\")\n",
    "    print(f\"Calculated Weights: {weights_tensor}\")\n",
    "    return weights_tensor"
   ],
   "id": "ac624b7af318bd88",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:40:44.900651Z",
     "start_time": "2025-10-30T09:40:40.926509Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 3. Define VLM (CLIP) Model and Logic ---\n",
    "\n",
    "# Load CLIP model and its preprocessing function\n",
    "print(f\"Loading CLIP model: {cfg.CLIP_MODEL} on {cfg.DEVICE}\")\n",
    "clip_model, preprocess = clip.load(cfg.CLIP_MODEL, device=cfg.DEVICE)\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model, preprocess = clip.load(\"ViT-B/32\", device=device)"
   ],
   "id": "b994a8b5996f4fa8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CLIP model: ViT-B/32 on cpu\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:40:56.170785Z",
     "start_time": "2025-10-30T09:40:56.165764Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# This section is the core of the VLM\n",
    "# Define Text Prompts\n",
    "text_prompts = [\n",
    "    \"An x-ray image of a normal case without a mass\",\n",
    "    \"An x-ray image containing a benign mass\",\n",
    "    \"An x-ray image containing a malignant mass\"\n",
    "]\n",
    "\n",
    "# Tokenize texts and send to device\n",
    "text_inputs = clip.tokenize(text_prompts).to(cfg.DEVICE)"
   ],
   "id": "4c20934f7e1c78d2",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:40:56.968993Z",
     "start_time": "2025-10-30T09:40:56.963492Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 4. Training and Validation Functions ---\n",
    "\n",
    "def train_one_epoch(model, dataloader, criterion, optimizer, text_features):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    pbar = tqdm(dataloader, desc=\"Training Epoch\")\n",
    "    for images, labels in pbar:\n",
    "        # Remove corrupted images (which have label -1)\n",
    "        valid_indices = labels != -1\n",
    "        if not valid_indices.any():\n",
    "            continue\n",
    "        \n",
    "        images = images[valid_indices].to(cfg.DEVICE)\n",
    "        labels = labels[valid_indices].to(cfg.DEVICE)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Extract image features\n",
    "        image_features = model.encode_image(images)\n",
    "        \n",
    "        # Normalize features (CLIP standard)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "        # Calculate similarity (logits)\n",
    "        # logit_scale is a learnable parameter in the CLIP model\n",
    "        logit_scale = model.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.T\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(logits_per_image, labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item() * images.size(0)\n",
    "        \n",
    "        # Store predictions and labels for F1\n",
    "        preds = logits_per_image.argmax(dim=1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        \n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n",
    "    epoch_f1_macro = f1_score(all_labels, all_preds, average='macro')\n",
    "    \n",
    "    return epoch_loss, epoch_f1_weighted, epoch_f1_macro"
   ],
   "id": "c303070036a5f88f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:40:57.482675Z",
     "start_time": "2025-10-30T09:40:57.477966Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def validate(model, dataloader, criterion, text_features):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        pbar = tqdm(dataloader, desc=\"Validating\")\n",
    "        for images, labels in pbar:\n",
    "            valid_indices = labels != -1\n",
    "            if not valid_indices.any():\n",
    "                continue\n",
    "\n",
    "            images = images[valid_indices].to(cfg.DEVICE)\n",
    "            labels = labels[valid_indices].to(cfg.DEVICE)\n",
    "            \n",
    "            # Extract features\n",
    "            image_features = model.encode_image(images)\n",
    "            image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "            \n",
    "            # Calculate logits\n",
    "            logit_scale = model.logit_scale.exp()\n",
    "            logits_per_image = logit_scale * image_features @ text_features.T\n",
    "            \n",
    "            loss = criterion(logits_per_image, labels)\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "\n",
    "            preds = logits_per_image.argmax(dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_loss = running_loss / len(dataloader.dataset)\n",
    "    epoch_f1_weighted = f1_score(all_labels, all_preds, average='weighted', zero_division=0)\n",
    "    epoch_f1_macro = f1_score(all_labels, all_preds, average='macro', zero_division=0)\n",
    "    \n",
    "    # Full report\n",
    "    report = classification_report(all_labels, all_preds, target_names=[\"normal\", \"benign\", \"malignant\"], zero_division=0)\n",
    "    \n",
    "    return epoch_loss, epoch_f1_weighted, epoch_f1_macro, report\n"
   ],
   "id": "f1e7441025c96fdf",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:40:58.313104Z",
     "start_time": "2025-10-30T09:40:58.108232Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 5. Main Execution Loop ---\n",
    "\n",
    "print(\"Starting data preparation...\")\n",
    "\n",
    "# 1. Prepare DataFrame\n",
    "full_df = prepare_dataframe(cfg.DATA_PATH)\n",
    "print(f\"Total images found: {len(full_df)}\")\n",
    "print(full_df['label'].value_counts())\n",
    "\n",
    "# 2. Split data into Train and Validation\n",
    "val_size = int(len(full_df) * cfg.VALIDATION_SPLIT)\n",
    "train_size = len(full_df) - val_size\n",
    "train_df, val_df = random_split(full_df, [train_size, val_size])\n",
    "\n",
    "# Convert subsets to Dataset\n",
    "train_dataset = XRayDataset(train_df.dataset.iloc[train_df.indices], preprocess)\n",
    "val_dataset = XRayDataset(val_df.dataset.iloc[val_df.indices], preprocess)\n",
    "\n",
    "print(f\"Train size: {len(train_dataset)}, Validation size: {len(val_dataset)}\")\n",
    "\n",
    "# 3. Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=cfg.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "# Note: This slows down data loading, but solves the multiprocessing issue.\n",
    "\n",
    "# 4. Calculate class weights (based on training data)\n",
    "print(\"Calculating class weights...\")\n",
    "class_weights = get_class_weights(train_df.dataset.iloc[train_df.indices])\n",
    "\n",
    "# 5. Define Loss Function and Optimizer\n",
    "# Use calculated weights to handle imbalance\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "optimizer = optim.Adam(clip_model.parameters(), lr=cfg.LEARNING_RATE)\n",
    "\n",
    "# Extract text features only once (since texts are constant)\n",
    "with torch.no_grad():\n",
    "    text_features = clip_model.encode_text(text_inputs)\n",
    "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)"
   ],
   "id": "ae178ce6cbd2b217",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preparation...\n",
      "Total images found: 624\n",
      "label\n",
      "benign       350\n",
      "malignant    168\n",
      "normal       106\n",
      "Name: count, dtype: int64\n",
      "Train size: 500, Validation size: 124\n",
      "Calculating class weights...\n",
      "Class Counts: [79, 298, 123]\n",
      "Calculated Weights: tensor([2.1097, 0.5593, 1.3550])\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:46:07.214952Z",
     "start_time": "2025-10-30T09:40:59.353753Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 6. Training and Validation Loop\n",
    "best_f1 = 0.0\n",
    "print(\"Starting model training...\")\n",
    "for epoch in range(cfg.EPOCHS):\n",
    "    print(f\"\\n--- Epoch {epoch+1}/{cfg.EPOCHS} ---\")\n",
    "    \n",
    "    train_loss, train_f1_w, train_f1_m = train_one_epoch(clip_model, train_loader, criterion, optimizer, text_features)\n",
    "    print(f\"Train Loss: {train_loss:.4f} | Weighted F1: {train_f1_w:.4f} | Macro F1: {train_f1_m:.4f}\")\n",
    "\n",
    "    val_loss, val_f1_w, val_f1_m, report = validate(clip_model, val_loader, criterion, text_features)\n",
    "    print(f\"Valid Loss: {val_loss:.4f} | Weighted F1: {val_f1_w:.4f} | Macro F1: {val_f1_m:.4f}\")\n",
    "    \n",
    "    # Save the best model based on Macro F1 (as it gives importance to minority classes)\n",
    "    if val_f1_m > best_f1:\n",
    "        best_f1 = val_f1_m\n",
    "        torch.save(clip_model.state_dict(), \"best_clip_classifier.pth\")\n",
    "        print(f\"New best model saved with Macro F1: {best_f1:.4f}\")\n",
    "        \n",
    "print(\"\\nTraining Finished.\")\n",
    "print(f\"Best Validation Macro F1 Score: {best_f1:.4f}\")\n",
    "\n",
    "# Load the best model and display the final report\n",
    "print(\"\\nLoading best model for final report on validation set...\")\n",
    "clip_model.load_state_dict(torch.load(\"best_clip_classifier.pth\"))\n",
    "_, _, _, final_report = validate(clip_model, val_loader, criterion, text_features)\n",
    "print(\"--- Final Validation Report ---\")\n",
    "print(final_report)"
   ],
   "id": "87725a0e18a52ef7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting model training...\n",
      "\n",
      "--- Epoch 1/10 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9edcc9da4b874826adfe2d5ebd01eac8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 1.0228 | Weighted F1: 0.5210 | Macro F1: 0.4793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5255fee865e74b31b1cd6623842beb6a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.8485 | Weighted F1: 0.5733 | Macro F1: 0.5696\n",
      "New best model saved with Macro F1: 0.5696\n",
      "\n",
      "--- Epoch 2/10 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "27d345a02b164c87819783efc03486ee"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.4968 | Weighted F1: 0.7869 | Macro F1: 0.7643\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d9e021909034a52b5c1df0740fa23e2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.6248 | Weighted F1: 0.7460 | Macro F1: 0.7408\n",
      "New best model saved with Macro F1: 0.7408\n",
      "\n",
      "--- Epoch 3/10 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6790091cb0dc4811b6a5299f5cc6b4b2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.3363 | Weighted F1: 0.8883 | Macro F1: 0.8673\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9da1e72f957a44b3908e6228c7afce70"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.6408 | Weighted F1: 0.7486 | Macro F1: 0.7500\n",
      "New best model saved with Macro F1: 0.7500\n",
      "\n",
      "--- Epoch 4/10 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3a9f8f990a3e4ab098cac4afb231a641"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1672 | Weighted F1: 0.9466 | Macro F1: 0.9374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "56881bedd37144d1b23c08db61c44f6b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.7289 | Weighted F1: 0.7740 | Macro F1: 0.7730\n",
      "New best model saved with Macro F1: 0.7730\n",
      "\n",
      "--- Epoch 5/10 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "08c9f8b9e7f946938797f5773ccd0f0e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.1683 | Weighted F1: 0.9548 | Macro F1: 0.9406\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5345e69c77994e1db7d5613613224e6c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.6557 | Weighted F1: 0.7500 | Macro F1: 0.7408\n",
      "\n",
      "--- Epoch 6/10 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "99602660064c4cbf80a6c08b9a0d28a8"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0589 | Weighted F1: 0.9900 | Macro F1: 0.9890\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d143a9958fbc4138aa963ffe94b83099"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.5689 | Weighted F1: 0.7788 | Macro F1: 0.7832\n",
      "New best model saved with Macro F1: 0.7832\n",
      "\n",
      "--- Epoch 7/10 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "545818b4be1d4ea1a9f1a2e5bf2136f0"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0282 | Weighted F1: 0.9880 | Macro F1: 0.9870\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "600b2c563596447fbc491b49683e126d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.6013 | Weighted F1: 0.7883 | Macro F1: 0.7946\n",
      "New best model saved with Macro F1: 0.7946\n",
      "\n",
      "--- Epoch 8/10 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f3235f094c4f42798aabd3741d38ff6e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0227 | Weighted F1: 0.9920 | Macro F1: 0.9916\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "39e5d63cbf7b4c9394cfe0392d64be20"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.8949 | Weighted F1: 0.7882 | Macro F1: 0.7934\n",
      "\n",
      "--- Epoch 9/10 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6ed98bdfd97340b193800ea0a287a050"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0162 | Weighted F1: 0.9960 | Macro F1: 0.9962\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "e052d98c3dd14b76b3a4dd9870cc26f5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.6758 | Weighted F1: 0.8151 | Macro F1: 0.8146\n",
      "New best model saved with Macro F1: 0.8146\n",
      "\n",
      "--- Epoch 10/10 ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Training Epoch:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3ef950c8f23e4faf8db6b722cbd01c57"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Loss: 0.0141 | Weighted F1: 0.9920 | Macro F1: 0.9924\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f195c705334b4fabbcee7dc981aba84c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid Loss: 0.7339 | Weighted F1: 0.7983 | Macro F1: 0.7999\n",
      "\n",
      "Training Finished.\n",
      "Best Validation Macro F1 Score: 0.8146\n",
      "\n",
      "Loading best model for final report on validation set...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Validating:   0%|          | 0/4 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e73e6fc1b874a09b5e218241f50dc50"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Final Validation Report ---\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      normal       0.71      0.93      0.81        27\n",
      "      benign       0.82      0.79      0.80        52\n",
      "   malignant       0.90      0.78      0.83        45\n",
      "\n",
      "    accuracy                           0.81       124\n",
      "   macro avg       0.81      0.83      0.81       124\n",
      "weighted avg       0.83      0.81      0.82       124\n",
      "\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:46:26.600384Z",
     "start_time": "2025-10-30T09:46:22.815290Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- 7. Predict on Test Data and Create Submission File ---\n",
    "\n",
    "# 1. Load Test DataFrame\n",
    "test_df_raw = pd.read_csv('test/test.csv')\n",
    "TEST_IMAGE_DIR = 'test/images/'\n",
    "\n",
    "# 2. Prepare Test DataFrame (including full image path)\n",
    "test_data = []\n",
    "for img_name in test_df_raw['image']:\n",
    "    test_data.append({\n",
    "        \"image_path\": os.path.join(TEST_IMAGE_DIR, img_name),\n",
    "        \"label\": \"unknown\" # unknown label\n",
    "    })\n",
    "test_full_df = pd.DataFrame(test_data)\n",
    "\n",
    "# 3. Define Test Dataset and DataLoader\n",
    "class TestXRayDataset(Dataset):\n",
    "    def __init__(self, dataframe, preprocess):\n",
    "        self.df = dataframe\n",
    "        self.preprocess = preprocess\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        image_path = row['image_path']\n",
    "        \n",
    "        try:\n",
    "            image = Image.open(image_path).convert(\"RGB\")\n",
    "            image_tensor = self.preprocess(image)\n",
    "            return image_tensor\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading test image {image_path}: {e}\")\n",
    "            return torch.zeros((3, 224, 224))\n",
    "\n",
    "test_dataset = TestXRayDataset(test_full_df, preprocess)\n",
    "test_loader = DataLoader(test_dataset, batch_size=cfg.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# 4. Run Prediction (Inference)\n",
    "clip_model.eval()\n",
    "all_predictions = []\n",
    "\n",
    "print(\"\\nStarting Test Data Prediction...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    for images in tqdm(test_loader, desc=\"Predicting\"):\n",
    "        images = images.to(cfg.DEVICE)\n",
    "        \n",
    "        # Extract features\n",
    "        image_features = clip_model.encode_image(images)\n",
    "        image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
    "        \n",
    "        # Calculate logits\n",
    "        logit_scale = clip_model.logit_scale.exp()\n",
    "        logits_per_image = logit_scale * image_features @ text_features.T\n",
    "        \n",
    "        preds = logits_per_image.argmax(dim=1)\n",
    "        all_predictions.extend(preds.cpu().numpy())\n",
    "\n",
    "# 5. Convert predictions to One-Hot and create final DataFrame\n",
    "idx_to_class = {0: 'normal', 1: 'benign', 2: 'malignant'}\n",
    "predictions_one_hot = np.zeros((len(all_predictions), 3), dtype=int)\n",
    "\n",
    "for i, pred_idx in enumerate(all_predictions):\n",
    "    if pred_idx == 0:\n",
    "        predictions_one_hot[i, 0] = 1 # normal\n",
    "    elif pred_idx == 1:\n",
    "        predictions_one_hot[i, 2] = 1 # benign (Note the order of the output columns)\n",
    "    elif pred_idx == 2:\n",
    "        predictions_one_hot[i, 1] = 1 # malignant\n",
    "\n",
    "# Create final DataFrame\n",
    "submission = pd.DataFrame({\n",
    "    'image': test_df_raw['image'],\n",
    "    'class_benign': predictions_one_hot[:, 2],\n",
    "    'class_malignant': predictions_one_hot[:, 1],\n",
    "    'class_normal': predictions_one_hot[:, 0]\n",
    "    \n",
    "})\n",
    "\n",
    "print(\"\\nSubmission DataFrame created successfully.\")\n",
    "print(submission.head())"
   ],
   "id": "62ce55636435722c",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting Test Data Prediction...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Predicting:   0%|          | 0/5 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "15591b67620541c6b2c6a355afc97d11"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Submission DataFrame created successfully.\n",
      "           image  class_benign  class_malignant  class_normal\n",
      "0  test_0001.png             0                0             1\n",
      "1  test_0002.png             1                0             0\n",
      "2  test_0003.png             1                0             0\n",
      "3  test_0004.png             1                0             0\n",
      "4  test_0005.png             0                1             0\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:46:29.986692Z",
     "start_time": "2025-10-30T09:46:29.966133Z"
    }
   },
   "cell_type": "code",
   "source": "submission",
   "id": "6717d8f0abb7883e",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "             image  class_benign  class_malignant  class_normal\n",
       "0    test_0001.png             0                0             1\n",
       "1    test_0002.png             1                0             0\n",
       "2    test_0003.png             1                0             0\n",
       "3    test_0004.png             1                0             0\n",
       "4    test_0005.png             0                1             0\n",
       "..             ...           ...              ...           ...\n",
       "151  test_0152.png             0                0             1\n",
       "152  test_0153.png             0                1             0\n",
       "153  test_0154.png             0                1             0\n",
       "154  test_0155.png             1                0             0\n",
       "155  test_0156.png             1                0             0\n",
       "\n",
       "[156 rows x 4 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>class_benign</th>\n",
       "      <th>class_malignant</th>\n",
       "      <th>class_normal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>test_0001.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>test_0002.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>test_0003.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>test_0004.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>test_0005.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>test_0152.png</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>test_0153.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>test_0154.png</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>test_0155.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>test_0156.png</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>156 rows Ã— 4 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "740a5756",
   "metadata": {},
   "source": [
    "<h2 dir=ltr align=left style=\"line-height:200%;font-family:sans-serif;color:#0099cc\">\n",
    "<font face=\"sans-serif\" color=\"#0099cc\">\n",
    "Evaluation Metric\n",
    "</font>\n",
    "</h2>\n",
    "\n",
    "<p dir=ltr style=\"direction: ltr; text-align: justify; line-height:200%; font-family:sans-serif; font-size:medium\">\n",
    "<font face=\"sans-serif\" size=3>\n",
    "    The metric we have chosen to evaluate the model's performance is called <code>F1_score</code>.\n",
    "    <br>\n",
    "    This metric is the standard for evaluating your model's quality. In other words, the judging system also uses this metric for scoring.\n",
    "    <br>\n",
    "    It is recommended to evaluate your model's performance on the training or validation set using this metric.\n",
    "</font>\n",
    "</p>\n",
    "\n",
    "<p dir=ltr style=\"direction: ltr; text-align: justify; line-height:200%; font-family:sans-serif; font-size:medium\">\n",
    "</font>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "id": "82bb3325",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:46:32.181314Z",
     "start_time": "2025-10-30T09:46:32.164448Z"
    }
   },
   "source": [
    "# --- Configuration for Segmentation ---\n",
    "class SegConfig:\n",
    "    DEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    IMAGE_SIZE = 256\n",
    "    ORIGINAL_SIZE = 500\n",
    "    BATCH_SIZE = 8\n",
    "    LEARNING_RATE = 1e-4\n",
    "    NUM_EPOCHS = 20\n",
    "    BACKBONE = 'resnet34' # U-Net model Encoder\n",
    "    ACTIVATION = 'sigmoid' # For binary mask output (0 to 1)\n",
    "\n",
    "seg_cfg = SegConfig()\n",
    "print(f\"Segmentation running on: {seg_cfg.DEVICE}\")\n",
    "\n",
    "# --- Helper functions explained in the previous guide ---\n",
    "\n",
    "# 1. Path collection function\n",
    "def create_seg_dataframe(base_dir='initial/train'):\n",
    "    data = []\n",
    "    classes = ['benign', 'malignant', 'normal']\n",
    "    for cls in classes:\n",
    "        image_dir = os.path.join(base_dir, cls, 'images')\n",
    "        mask_dir = os.path.join(base_dir, cls, 'masks')\n",
    "        image_paths = glob.glob(os.path.join(image_dir, '*.png'))\n",
    "        \n",
    "        for img_path in image_paths:\n",
    "            img_name = os.path.basename(img_path)\n",
    "            if cls == 'normal':\n",
    "                mask_path = None # All-black mask\n",
    "            else:\n",
    "                base_name = os.path.splitext(img_name)[0]\n",
    "                mask_path = os.path.join(mask_dir, f\"{base_name}_mask.png\")\n",
    "            \n",
    "            data.append({'image_path': img_path, 'mask_path': mask_path, 'class': cls})\n",
    "    return pd.DataFrame(data)\n",
    "\n",
    "# 2. Dataset class for segmentation\n",
    "class SegmentationXRayDataset(Dataset):\n",
    "    def __init__(self, dataframe, transform=None):\n",
    "        self.df = dataframe\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "        # Load images as NumPy arrays so Albumentations can process them\n",
    "        image = np.array(Image.open(row['image_path']).convert(\"RGB\"))\n",
    "\n",
    "        if row['class'] == 'normal' or row['mask_path'] is None:\n",
    "            # Create an all-black mask (0) for normal or test images\n",
    "            mask = np.zeros(image.shape[:2], dtype=np.uint8)\n",
    "        else:\n",
    "            # Load mask and ensure it is only 0 and 1\n",
    "            mask = np.array(Image.open(row['mask_path']).convert(\"L\"))\n",
    "            mask[mask > 0] = 1 # Tumor white (1), background black (0)\n",
    "\n",
    "        # Apply transformations (simultaneously on image and mask)\n",
    "        if self.transform:\n",
    "            augmented = self.transform(image=image, mask=mask)\n",
    "            image = augmented['image']\n",
    "            mask = augmented['mask']\n",
    "        \n",
    "        # Output: RGB image (3 channels) and binary mask (1 channel)\n",
    "        return image, mask.float().unsqueeze(0) \n",
    "\n",
    "# 3. Define Combined Loss (Dice + BCE)\n",
    "class DiceLoss(nn.Module):\n",
    "    # (Implementation provided in previous cells)\n",
    "    def __init__(self, smooth=1e-6):\n",
    "        super(DiceLoss, self).__init__()\n",
    "        self.smooth = smooth\n",
    "    def forward(self, inputs, targets):\n",
    "        inputs = inputs.view(-1)\n",
    "        targets = targets.view(-1)\n",
    "        intersection = (inputs * targets).sum()\n",
    "        dice = (2. * intersection + self.smooth) / (inputs.sum() + targets.sum() + self.smooth)\n",
    "        return 1 - dice\n",
    "\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, weight=0.5):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.bce = nn.BCEWithLogitsLoss()\n",
    "        self.dice = DiceLoss()\n",
    "        self.weight = weight \n",
    "\n",
    "    def forward(self, inputs, targets):\n",
    "        # BCEWithLogitsLoss uses raw output (Logits)\n",
    "        bce_loss = self.bce(inputs, targets) \n",
    "        # Dice Loss uses Sigmoid output\n",
    "        dice_loss = self.dice(torch.sigmoid(inputs), targets)\n",
    "        return self.weight * bce_loss + (1 - self.weight) * dice_loss"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation running on: cpu\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:46:32.980981Z",
     "start_time": "2025-10-30T09:46:32.960298Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Prepare Training Data ---\n",
    "full_train_df = create_seg_dataframe(base_dir='train')\n",
    "full_train_df"
   ],
   "id": "d79f614996b2d224",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "                               image_path  \\\n",
       "0      train\\benign\\images\\benign (1).png   \n",
       "1    train\\benign\\images\\benign (100).png   \n",
       "2    train\\benign\\images\\benign (101).png   \n",
       "3    train\\benign\\images\\benign (102).png   \n",
       "4    train\\benign\\images\\benign (103).png   \n",
       "..                                    ...   \n",
       "619   train\\normal\\images\\normal (95).png   \n",
       "620   train\\normal\\images\\normal (96).png   \n",
       "621   train\\normal\\images\\normal (97).png   \n",
       "622   train\\normal\\images\\normal (98).png   \n",
       "623   train\\normal\\images\\normal (99).png   \n",
       "\n",
       "                                    mask_path   class  \n",
       "0      train\\benign\\masks\\benign (1)_mask.png  benign  \n",
       "1    train\\benign\\masks\\benign (100)_mask.png  benign  \n",
       "2    train\\benign\\masks\\benign (101)_mask.png  benign  \n",
       "3    train\\benign\\masks\\benign (102)_mask.png  benign  \n",
       "4    train\\benign\\masks\\benign (103)_mask.png  benign  \n",
       "..                                        ...     ...  \n",
       "619                                      None  normal  \n",
       "620                                      None  normal  \n",
       "621                                      None  normal  \n",
       "622                                      None  normal  \n",
       "623                                      None  normal  \n",
       "\n",
       "[624 rows x 3 columns]"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_path</th>\n",
       "      <th>mask_path</th>\n",
       "      <th>class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>train\\benign\\images\\benign (1).png</td>\n",
       "      <td>train\\benign\\masks\\benign (1)_mask.png</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>train\\benign\\images\\benign (100).png</td>\n",
       "      <td>train\\benign\\masks\\benign (100)_mask.png</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>train\\benign\\images\\benign (101).png</td>\n",
       "      <td>train\\benign\\masks\\benign (101)_mask.png</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>train\\benign\\images\\benign (102).png</td>\n",
       "      <td>train\\benign\\masks\\benign (102)_mask.png</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>train\\benign\\images\\benign (103).png</td>\n",
       "      <td>train\\benign\\masks\\benign (103)_mask.png</td>\n",
       "      <td>benign</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>619</th>\n",
       "      <td>train\\normal\\images\\normal (95).png</td>\n",
       "      <td>None</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>620</th>\n",
       "      <td>train\\normal\\images\\normal (96).png</td>\n",
       "      <td>None</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>621</th>\n",
       "      <td>train\\normal\\images\\normal (97).png</td>\n",
       "      <td>None</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>622</th>\n",
       "      <td>train\\normal\\images\\normal (98).png</td>\n",
       "      <td>None</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>623</th>\n",
       "      <td>train\\normal\\images\\normal (99).png</td>\n",
       "      <td>None</td>\n",
       "      <td>normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>624 rows Ã— 3 columns</p>\n",
       "</div>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:46:37.465193Z",
     "start_time": "2025-10-30T09:46:37.445972Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Split data into training (80%) and validation (20%)\n",
    "train_size = int(0.8 * len(full_train_df))\n",
    "val_size = len(full_train_df) - train_size\n",
    "train_df, val_df = random_split(full_train_df, [train_size, val_size])\n",
    "# Convert to DataFrame for easier indexing\n",
    "train_df = full_train_df.iloc[train_df.indices].reset_index(drop=True)\n",
    "val_df = full_train_df.iloc[val_df.indices].reset_index(drop=True)"
   ],
   "id": "b7a7800aa8906036",
   "outputs": [],
   "execution_count": 21
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:46:37.917856Z",
     "start_time": "2025-10-30T09:46:37.888712Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Define Transforms ---\n",
    "train_transform_seg = A.Compose([\n",
    "    A.Resize(seg_cfg.IMAGE_SIZE, seg_cfg.IMAGE_SIZE),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.Rotate(limit=15, p=0.5),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])\n",
    "\n",
    "val_transform_seg = A.Compose([\n",
    "    A.Resize(seg_cfg.IMAGE_SIZE, seg_cfg.IMAGE_SIZE),\n",
    "    A.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "    ToTensorV2(),\n",
    "])"
   ],
   "id": "fd1bd2d8b907cd06",
   "outputs": [],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:46:38.308750Z",
     "start_time": "2025-10-30T09:46:38.289936Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Create DataLoaders ---\n",
    "train_dataset_seg = SegmentationXRayDataset(train_df, transform=train_transform_seg)\n",
    "val_dataset_seg = SegmentationXRayDataset(val_df, transform=val_transform_seg)\n",
    "\n",
    "train_loader_seg = DataLoader(train_dataset_seg, batch_size=seg_cfg.BATCH_SIZE, shuffle=True, num_workers=0)\n",
    "val_loader_seg = DataLoader(val_dataset_seg, batch_size=seg_cfg.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Train samples: {len(train_dataset_seg)}, Validation samples: {len(val_dataset_seg)}\")"
   ],
   "id": "1d31adb0505f91c8",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train samples: 499, Validation samples: 125\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:46:38.980342Z",
     "start_time": "2025-10-30T09:46:38.722710Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Define Model, Loss, and Optimizer ---\n",
    "unet_model = smp.Unet(\n",
    "    encoder_name=seg_cfg.BACKBONE,     # Pre-trained ResNet34\n",
    "    encoder_weights=\"imagenet\",        # Use ImageNet weights\n",
    "    in_channels=3,                     # RGB input\n",
    "    classes=1,                         # Single-channel output (mask)\n",
    "    activation=None,                   # To use Logits in BCEWithLogitsLoss\n",
    ").to(seg_cfg.DEVICE)\n",
    "\n",
    "loss_fn_seg = CombinedLoss(weight=0.5)\n",
    "optimizer_seg = optim.Adam(unet_model.parameters(), lr=seg_cfg.LEARNING_RATE)\n",
    "scheduler_seg = optim.lr_scheduler.ReduceLROnPlateau(optimizer_seg, mode='min', factor=0.1, patience=5)"
   ],
   "id": "9868593e8b799458",
   "outputs": [],
   "execution_count": 24
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T09:46:39.062323Z",
     "start_time": "2025-10-30T09:46:39.044670Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Evaluation Function ---\n",
    "# IoU (Intersection over Union) metric for segmentation performance\n",
    "def iou_metric(preds, targets):\n",
    "    preds = (preds > 0.5).float() # Convert to binary mask\n",
    "    intersection = (preds * targets).sum()\n",
    "    union = (preds + targets).sum() - intersection\n",
    "    iou = (intersection + 1e-6) / (union + 1e-6)\n",
    "    return iou.mean()"
   ],
   "id": "c7282e076ec723ca",
   "outputs": [],
   "execution_count": 25
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T10:09:21.439247Z",
     "start_time": "2025-10-30T09:46:42.167308Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Training Loop ---\n",
    "best_val_iou = 0\n",
    "print(\"\\n--- Starting U-Net Training ---\")\n",
    "for epoch in range(seg_cfg.NUM_EPOCHS):\n",
    "    # Train Loop\n",
    "    unet_model.train()\n",
    "    train_loss = 0\n",
    "    for images, masks in tqdm(train_loader_seg, desc=f\"Epoch {epoch+1} Train\"):\n",
    "        images = images.to(seg_cfg.DEVICE)\n",
    "        masks = masks.to(seg_cfg.DEVICE)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = unet_model(images)\n",
    "        loss = loss_fn_seg(outputs, masks)\n",
    "        \n",
    "        # Backward pass and optimization\n",
    "        optimizer_seg.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_seg.step()\n",
    "        \n",
    "        train_loss += loss.item()\n",
    "    \n",
    "    avg_train_loss = train_loss / len(train_loader_seg)\n",
    "\n",
    "    # Validation Loop\n",
    "    unet_model.eval()\n",
    "    val_loss = 0\n",
    "    val_iou = 0\n",
    "    with torch.no_grad():\n",
    "        for images, masks in tqdm(val_loader_seg, desc=f\"Epoch {epoch+1} Val\"):\n",
    "            images = images.to(seg_cfg.DEVICE)\n",
    "            masks = masks.to(seg_cfg.DEVICE)\n",
    "            \n",
    "            outputs = unet_model(images)\n",
    "            loss = loss_fn_seg(outputs, masks)\n",
    "            val_loss += loss.item()\n",
    "            \n",
    "            # Calculate IoU\n",
    "            sigmoid_outputs = torch.sigmoid(outputs)\n",
    "            val_iou += iou_metric(sigmoid_outputs, masks).item()\n",
    "            \n",
    "    avg_val_loss = val_loss / len(val_loader_seg)\n",
    "    avg_val_iou = val_iou / len(val_loader_seg)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{seg_cfg.NUM_EPOCHS}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val IoU: {avg_val_iou:.4f}\")\n",
    "\n",
    "    # Update learning scheduler and save best model\n",
    "    scheduler_seg.step(avg_val_loss)\n",
    "\n",
    "    if avg_val_iou > best_val_iou:\n",
    "        best_val_iou = avg_val_iou\n",
    "        torch.save(unet_model.state_dict(), 'best_unet_model.pth')\n",
    "        print(f\"  --> Model saved with improved IoU: {best_val_iou:.4f}\")\n",
    "\n",
    "print(\"\\n--- U-Net Training Complete ---\")"
   ],
   "id": "52873e41836d74f6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting U-Net Training ---\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0ad2b65906dc4221b2884ab4087e26a5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 1 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5fd6f5559f214d3cab1906172eb13624"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20: Train Loss: 0.7152, Val Loss: 0.6192, Val IoU: 0.3426\n",
      "  --> Model saved with improved IoU: 0.3426\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "048455c0f6064cfbb4dd707016733ba9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 2 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab47d05af735457986dfe5eb8f289def"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/20: Train Loss: 0.5785, Val Loss: 0.5319, Val IoU: 0.4493\n",
      "  --> Model saved with improved IoU: 0.4493\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4a4392d1694e40898cfa9524ecd5fe8e"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 3 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ab311f3161f545d48e6109564c9b5ff6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/20: Train Loss: 0.5039, Val Loss: 0.4721, Val IoU: 0.4872\n",
      "  --> Model saved with improved IoU: 0.4872\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cb894ab5693e4b138c6d50093cad7acd"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 4 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "75bf09510dab4ef5b794ae5048d233c7"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/20: Train Loss: 0.4353, Val Loss: 0.4222, Val IoU: 0.5135\n",
      "  --> Model saved with improved IoU: 0.5135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2c07aa6073304415a4b357b6b1bbae3b"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 5 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a470450fddae439a85e7b9f5b75b6405"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/20: Train Loss: 0.3821, Val Loss: 0.3884, Val IoU: 0.5338\n",
      "  --> Model saved with improved IoU: 0.5338\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 6 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ef090219b59e4e9a9a0e777a9c19fe88"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 6 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "f70c8b1a2c744ee88671f75bff90adbb"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/20: Train Loss: 0.3498, Val Loss: 0.3593, Val IoU: 0.5252\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 7 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4ec29b4de4a74b97b58a70a80f1b731d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 7 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "854de44434224e459a8602485baabe24"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/20: Train Loss: 0.3068, Val Loss: 0.3468, Val IoU: 0.5074\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 8 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "bbd4a475934e4a4aa03581c70246d11c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 8 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "0204715f64a7438180c94a6100c93490"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/20: Train Loss: 0.2744, Val Loss: 0.3268, Val IoU: 0.5063\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 9 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "18bea6aca6a544318dcfddde78221348"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 9 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "131e87ff702d41c8866c361be7475804"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/20: Train Loss: 0.2426, Val Loss: 0.3034, Val IoU: 0.5317\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 10 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9c0d8064592b45b58d75d2a5d968a344"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 10 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "eff9d01452574a91bd6f0e08d642e9ce"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/20: Train Loss: 0.2263, Val Loss: 0.3076, Val IoU: 0.5134\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 11 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "a817d95c657340beaadba758155e0565"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 11 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3e42880934eb4ffdb394abd6831f50cc"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/20: Train Loss: 0.2145, Val Loss: 0.2862, Val IoU: 0.5244\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 12 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "529a8e8a5b124967939acca9e303c82d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 12 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3d23cdb112884405b6101d7abb388250"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/20: Train Loss: 0.1878, Val Loss: 0.2686, Val IoU: 0.5560\n",
      "  --> Model saved with improved IoU: 0.5560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 13 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fed39ffb24fe4fd0a367b8b85dff1e72"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 13 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3dd2c8e81e53441a8a149e5a3734f3b6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/20: Train Loss: 0.1694, Val Loss: 0.2687, Val IoU: 0.5484\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 14 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "884bff7e3dd24962b4b271e4159c4b98"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 14 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9d4f7e54e8994ddc8c82d1702cc66eb6"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/20: Train Loss: 0.1620, Val Loss: 0.2552, Val IoU: 0.5483\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 15 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "035cf2f25a1a439bbeb5d31123c48ed5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 15 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c18c16dbd96543378657c248b3785090"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/20: Train Loss: 0.1458, Val Loss: 0.2549, Val IoU: 0.5514\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 16 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1a4960733e4b4587b5d995f7b7b5eaa5"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 16 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "c424b4166c54455d9a921a374647cdfe"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/20: Train Loss: 0.1315, Val Loss: 0.2463, Val IoU: 0.5653\n",
      "  --> Model saved with improved IoU: 0.5653\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 17 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "215cb55eee94443fbf7d16298d5cb57a"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 17 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "cdfe9ce4249041f58c9aae9f7f785222"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/20: Train Loss: 0.1337, Val Loss: 0.2398, Val IoU: 0.5646\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 18 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "029bcb6c07d84009b1964f34cce82405"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 18 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "19082eda144942cc8c01fe02136b8ca3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/20: Train Loss: 0.1360, Val Loss: 0.2312, Val IoU: 0.5834\n",
      "  --> Model saved with improved IoU: 0.5834\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 19 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d1a4a3e6df6a4a38a798972b828ec4e3"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 19 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "62c37251ab4046ad888429c04f3d26a9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/20: Train Loss: 0.1518, Val Loss: 0.2376, Val IoU: 0.5680\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Epoch 20 Train:   0%|          | 0/63 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8603971437c4d0783503244ada3fb33"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Epoch 20 Val:   0%|          | 0/16 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "9efd6e39ce0647c88250bd45de4a810d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: Train Loss: 0.1342, Val Loss: 0.2387, Val IoU: 0.5671\n",
      "\n",
      "--- U-Net Training Complete ---\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T10:09:21.660886Z",
     "start_time": "2025-10-30T10:09:21.440263Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Load the best-trained model ---\n",
    "best_unet_model = smp.Unet(\n",
    "    encoder_name=seg_cfg.BACKBONE,\n",
    "    encoder_weights=None, # We will load weights from the file\n",
    "    in_channels=3,\n",
    "    classes=1,\n",
    "    activation=None,\n",
    ").to(seg_cfg.DEVICE)\n",
    "best_unet_model.load_state_dict(torch.load('best_unet_model.pth'))"
   ],
   "id": "a2011e52b5ea9fb7",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 27
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T10:09:21.667068Z",
     "start_time": "2025-10-30T10:09:21.661900Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_original_image_sizes(test_df_raw, image_dir):\n",
    "    \"\"\"Returns a map of image name to original dimensions (width, height).\"\"\"\n",
    "    size_map = {}\n",
    "    for img_name in test_df_raw['image']:\n",
    "        img_path = os.path.join(image_dir, img_name)\n",
    "        try:\n",
    "            # Open image to read dimensions\n",
    "            with Image.open(img_path) as img:\n",
    "                size_map[img_name] = img.size # (width, height)\n",
    "        except Exception as e:\n",
    "            # If reading fails, use default dimensions (which was 500x500)\n",
    "            print(f\"Warning: Could not read image size for {img_name}. Using default 500x500. Error: {e}\")\n",
    "            size_map[img_name] = (500, 500) # Fallback\n",
    "    return size_map"
   ],
   "id": "167fe11bc84f0e02",
   "outputs": [],
   "execution_count": 28
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T10:09:29.713406Z",
     "start_time": "2025-10-30T10:09:21.668082Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Define final prediction function (modified) ---\n",
    "def predict_segmentation(model, test_loader, device, output_dir='segmentation_submission_masks'):\n",
    "    \"\"\"Generates, saves, and resizes predicted masks to original dimensions.\"\"\"\n",
    "    model.eval()\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Load Test DataFrame\n",
    "    test_df_raw = pd.read_csv('test/test.csv') \n",
    "    \n",
    "    # A) Collect names and original dimensions\n",
    "    image_names = test_df_raw['image'].tolist()\n",
    "    original_sizes_map = get_original_image_sizes(test_df_raw, TEST_IMAGE_DIR)\n",
    "\n",
    "    print(f\"Starting prediction and saving masks to {output_dir}/...\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Prediction loop (images: image tensor, _: dummy mask tensor)\n",
    "        for i, (images, _) in enumerate(tqdm(test_loader, desc=\"Predicting Test Masks\")):\n",
    "            \n",
    "            images = images.to(device)\n",
    "            \n",
    "            # 1. Predict (Logits)\n",
    "            outputs = model(images)\n",
    "            \n",
    "            # 2. Convert to binary\n",
    "            predictions = torch.sigmoid(outputs) > 0.5\n",
    "            # Convert to 0 or 255\n",
    "            predictions = predictions.cpu().numpy().astype(np.uint8) * 255 \n",
    "\n",
    "            # 3. Save masks with original dimensions\n",
    "            batch_size = predictions.shape[0]\n",
    "            for j in range(batch_size):\n",
    "                mask = predictions[j, 0, :, :] # (1, H, W) -> (H, W)\n",
    "                \n",
    "                # Find image name and original dimensions\n",
    "                idx_in_df = i * test_loader.batch_size + j\n",
    "                img_name = image_names[idx_in_df]\n",
    "                original_width, original_height = original_sizes_map[img_name]\n",
    "                \n",
    "                # Resizing to original dimensions (W, H)\n",
    "                mask_pil = Image.fromarray(mask)\n",
    "                # Use Image.NEAREST to resize pixels without color interpolation\n",
    "                mask_pil = mask_pil.resize(\n",
    "                    (original_width, original_height), \n",
    "                    resample=Image.NEAREST\n",
    "                ) \n",
    "                \n",
    "                # Save\n",
    "                base_name = os.path.splitext(img_name)[0]\n",
    "                output_mask_name = f\"{base_name}_mask.png\"\n",
    "                mask_pil.save(os.path.join(output_dir, output_mask_name))\n",
    "\n",
    "# --- Prepare test data for segmentation ---\n",
    "\n",
    "TEST_IMAGE_DIR = 'test/images/' # Test data path\n",
    "\n",
    "# Create Test DataFrame (no mask needed)\n",
    "test_data_seg = []\n",
    "test_df_raw = pd.read_csv('test/test.csv')\n",
    "for img_name in test_df_raw['image']:\n",
    "    test_data_seg.append({\n",
    "        \"image_path\": os.path.join(TEST_IMAGE_DIR, img_name),\n",
    "        \"mask_path\": None, # Mask not available\n",
    "        \"class\": \"unknown\"\n",
    "    })\n",
    "test_full_df_seg = pd.DataFrame(test_data_seg)\n",
    "\n",
    "# Define Test DataLoader (without Augmentation)\n",
    "test_dataset_seg = SegmentationXRayDataset(test_full_df_seg, transform=val_transform_seg)\n",
    "test_loader_seg = DataLoader(test_dataset_seg, batch_size=seg_cfg.BATCH_SIZE, shuffle=False, num_workers=0)\n",
    "\n",
    "\n",
    "# --- Call prediction function ---\n",
    "predict_segmentation(\n",
    "    model=best_unet_model, \n",
    "    test_loader=test_loader_seg, \n",
    "    device=seg_cfg.DEVICE, \n",
    "    output_dir='segmentation'\n",
    ")\n",
    "\n",
    "print(\"\\n--- Prediction Complete ---\")\n",
    "print(\"All predicted masks are saved in the 'segmentation' folder.\")"
   ],
   "id": "3f78febc19d4e5c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting prediction and saving masks to segmentation/...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Predicting Test Masks:   0%|          | 0/20 [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2f6396ab6c87439a80bb68c915dbc1f4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Prediction Complete ---\n",
      "All predicted masks are saved in the 'segmentation' folder.\n"
     ]
    }
   ],
   "execution_count": 29
  },
  {
   "cell_type": "code",
   "id": "def86ad1a6543f5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T10:09:29.720971Z",
     "start_time": "2025-10-30T10:09:29.715971Z"
    }
   },
   "source": [
    "# This command is specific to Jupyter environments to save the notebook file.\n",
    "# You don't need to run the following cells (its whole purpose was only for making a submission zipped file for the contest)\n",
    "\n",
    "def compress(file_names):\n",
    "    print(\"File Paths to be zipped:\")\n",
    "    print(file_names)\n",
    "    compression = zipfile.ZIP_DEFLATED\n",
    "    with zipfile.ZipFile(\"result.zip\", mode=\"w\") as zf:\n",
    "        for file_name in file_names:\n",
    "            if os.path.exists(file_name):\n",
    "                zf.write(file_name, arcname=os.path.basename(file_name), compress_type=compression)\n",
    "            else:\n",
    "                print(f\"Warning: File not found and will not be added to zip: {file_name}\")"
   ],
   "outputs": [],
   "execution_count": 30
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-10-30T10:09:29.817170Z",
     "start_time": "2025-10-30T10:09:29.722108Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# --- Create final submission files ---\n",
    "\n",
    "# 1. Save the classification results DataFrame to a CSV file\n",
    "submission.to_csv('submission.csv', index=False)\n",
    "\n",
    "# 2. Define the list of files for the zip archive\n",
    "file_names = ['Breast-Cancer-DetSeg.ipynb', 'submission.csv']\n",
    "\n",
    "# 3. Define the list of mask files\n",
    "# Assuming mask files are in 'segmentation' folder\n",
    "segmentation_dir = 'segmentation'\n",
    "if os.path.exists(segmentation_dir):\n",
    "    mask_files = [os.path.join(segmentation_dir, f) for f in os.listdir(segmentation_dir) if f.endswith('_mask.png')]\n",
    "    file_names.extend(mask_files)\n",
    "\n",
    "# 4. Create the zip file\n",
    "compress(file_names)\n",
    "print(\"Submission file 'result.zip' created successfully!\")"
   ],
   "id": "2bed8e9250bfcbd4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Paths to be zipped:\n",
      "['Breast-Cancer-DetSeg.ipynb', 'submission.csv', 'segmentation\\\\test_0001_mask.png', 'segmentation\\\\test_0002_mask.png', 'segmentation\\\\test_0003_mask.png', 'segmentation\\\\test_0004_mask.png', 'segmentation\\\\test_0005_mask.png', 'segmentation\\\\test_0006_mask.png', 'segmentation\\\\test_0007_mask.png', 'segmentation\\\\test_0008_mask.png', 'segmentation\\\\test_0009_mask.png', 'segmentation\\\\test_0010_mask.png', 'segmentation\\\\test_0011_mask.png', 'segmentation\\\\test_0012_mask.png', 'segmentation\\\\test_0013_mask.png', 'segmentation\\\\test_0014_mask.png', 'segmentation\\\\test_0015_mask.png', 'segmentation\\\\test_0016_mask.png', 'segmentation\\\\test_0017_mask.png', 'segmentation\\\\test_0018_mask.png', 'segmentation\\\\test_0019_mask.png', 'segmentation\\\\test_0020_mask.png', 'segmentation\\\\test_0021_mask.png', 'segmentation\\\\test_0022_mask.png', 'segmentation\\\\test_0023_mask.png', 'segmentation\\\\test_0024_mask.png', 'segmentation\\\\test_0025_mask.png', 'segmentation\\\\test_0026_mask.png', 'segmentation\\\\test_0027_mask.png', 'segmentation\\\\test_0028_mask.png', 'segmentation\\\\test_0029_mask.png', 'segmentation\\\\test_0030_mask.png', 'segmentation\\\\test_0031_mask.png', 'segmentation\\\\test_0032_mask.png', 'segmentation\\\\test_0033_mask.png', 'segmentation\\\\test_0034_mask.png', 'segmentation\\\\test_0035_mask.png', 'segmentation\\\\test_0036_mask.png', 'segmentation\\\\test_0037_mask.png', 'segmentation\\\\test_0038_mask.png', 'segmentation\\\\test_0039_mask.png', 'segmentation\\\\test_0040_mask.png', 'segmentation\\\\test_0041_mask.png', 'segmentation\\\\test_0042_mask.png', 'segmentation\\\\test_0043_mask.png', 'segmentation\\\\test_0044_mask.png', 'segmentation\\\\test_0045_mask.png', 'segmentation\\\\test_0046_mask.png', 'segmentation\\\\test_0047_mask.png', 'segmentation\\\\test_0048_mask.png', 'segmentation\\\\test_0049_mask.png', 'segmentation\\\\test_0050_mask.png', 'segmentation\\\\test_0051_mask.png', 'segmentation\\\\test_0052_mask.png', 'segmentation\\\\test_0053_mask.png', 'segmentation\\\\test_0054_mask.png', 'segmentation\\\\test_0055_mask.png', 'segmentation\\\\test_0056_mask.png', 'segmentation\\\\test_0057_mask.png', 'segmentation\\\\test_0058_mask.png', 'segmentation\\\\test_0059_mask.png', 'segmentation\\\\test_0060_mask.png', 'segmentation\\\\test_0061_mask.png', 'segmentation\\\\test_0062_mask.png', 'segmentation\\\\test_0063_mask.png', 'segmentation\\\\test_0064_mask.png', 'segmentation\\\\test_0065_mask.png', 'segmentation\\\\test_0066_mask.png', 'segmentation\\\\test_0067_mask.png', 'segmentation\\\\test_0068_mask.png', 'segmentation\\\\test_0069_mask.png', 'segmentation\\\\test_0070_mask.png', 'segmentation\\\\test_0071_mask.png', 'segmentation\\\\test_0072_mask.png', 'segmentation\\\\test_0073_mask.png', 'segmentation\\\\test_0074_mask.png', 'segmentation\\\\test_0075_mask.png', 'segmentation\\\\test_0076_mask.png', 'segmentation\\\\test_0077_mask.png', 'segmentation\\\\test_0078_mask.png', 'segmentation\\\\test_0079_mask.png', 'segmentation\\\\test_0080_mask.png', 'segmentation\\\\test_0081_mask.png', 'segmentation\\\\test_0082_mask.png', 'segmentation\\\\test_0083_mask.png', 'segmentation\\\\test_0084_mask.png', 'segmentation\\\\test_0085_mask.png', 'segmentation\\\\test_0086_mask.png', 'segmentation\\\\test_0087_mask.png', 'segmentation\\\\test_0088_mask.png', 'segmentation\\\\test_0089_mask.png', 'segmentation\\\\test_0090_mask.png', 'segmentation\\\\test_0091_mask.png', 'segmentation\\\\test_0092_mask.png', 'segmentation\\\\test_0093_mask.png', 'segmentation\\\\test_0094_mask.png', 'segmentation\\\\test_0095_mask.png', 'segmentation\\\\test_0096_mask.png', 'segmentation\\\\test_0097_mask.png', 'segmentation\\\\test_0098_mask.png', 'segmentation\\\\test_0099_mask.png', 'segmentation\\\\test_0100_mask.png', 'segmentation\\\\test_0101_mask.png', 'segmentation\\\\test_0102_mask.png', 'segmentation\\\\test_0103_mask.png', 'segmentation\\\\test_0104_mask.png', 'segmentation\\\\test_0105_mask.png', 'segmentation\\\\test_0106_mask.png', 'segmentation\\\\test_0107_mask.png', 'segmentation\\\\test_0108_mask.png', 'segmentation\\\\test_0109_mask.png', 'segmentation\\\\test_0110_mask.png', 'segmentation\\\\test_0111_mask.png', 'segmentation\\\\test_0112_mask.png', 'segmentation\\\\test_0113_mask.png', 'segmentation\\\\test_0114_mask.png', 'segmentation\\\\test_0115_mask.png', 'segmentation\\\\test_0116_mask.png', 'segmentation\\\\test_0117_mask.png', 'segmentation\\\\test_0118_mask.png', 'segmentation\\\\test_0119_mask.png', 'segmentation\\\\test_0120_mask.png', 'segmentation\\\\test_0121_mask.png', 'segmentation\\\\test_0122_mask.png', 'segmentation\\\\test_0123_mask.png', 'segmentation\\\\test_0124_mask.png', 'segmentation\\\\test_0125_mask.png', 'segmentation\\\\test_0126_mask.png', 'segmentation\\\\test_0127_mask.png', 'segmentation\\\\test_0128_mask.png', 'segmentation\\\\test_0129_mask.png', 'segmentation\\\\test_0130_mask.png', 'segmentation\\\\test_0131_mask.png', 'segmentation\\\\test_0132_mask.png', 'segmentation\\\\test_0133_mask.png', 'segmentation\\\\test_0134_mask.png', 'segmentation\\\\test_0135_mask.png', 'segmentation\\\\test_0136_mask.png', 'segmentation\\\\test_0137_mask.png', 'segmentation\\\\test_0138_mask.png', 'segmentation\\\\test_0139_mask.png', 'segmentation\\\\test_0140_mask.png', 'segmentation\\\\test_0141_mask.png', 'segmentation\\\\test_0142_mask.png', 'segmentation\\\\test_0143_mask.png', 'segmentation\\\\test_0144_mask.png', 'segmentation\\\\test_0145_mask.png', 'segmentation\\\\test_0146_mask.png', 'segmentation\\\\test_0147_mask.png', 'segmentation\\\\test_0148_mask.png', 'segmentation\\\\test_0149_mask.png', 'segmentation\\\\test_0150_mask.png', 'segmentation\\\\test_0151_mask.png', 'segmentation\\\\test_0152_mask.png', 'segmentation\\\\test_0153_mask.png', 'segmentation\\\\test_0154_mask.png', 'segmentation\\\\test_0155_mask.png', 'segmentation\\\\test_0156_mask.png']\n",
      "Submission file 'result.zip' created successfully!\n"
     ]
    }
   ],
   "execution_count": 31
  }
 ]
}
